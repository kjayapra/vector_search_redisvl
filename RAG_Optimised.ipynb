{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53156d2f-baad-4bc6-b716-3b68b19efbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Optional\n",
    "from redis import Redis\n",
    "from redisvl.utils.vectorize import HFTextVectorizer\n",
    "from redisvl.index import AsyncSearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.extensions.cache.llm import SemanticCache\n",
    "from redisvl.extensions.session import SemanticMessageHistory\n",
    "import openai\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Redis connection\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Cache settings\n",
    "CACHE_TTL = 300  # 5 minutes\n",
    "CACHE_DISTANCE_THRESHOLD = 0.2\n",
    "\n",
    "# Session settings\n",
    "SESSION_TTL = 3600  # 1 hour\n",
    "NUM_RESULTS = 5\n",
    "\n",
    "# ============================================\n",
    "# SEMANTIC CACHE SETUP\n",
    "# ============================================\n",
    "\n",
    "def setup_semantic_cache(vectorizer: HFTextVectorizer) -> SemanticCache:\n",
    "    \"\"\"\n",
    "    Initialize semantic cache for LLM responses\n",
    "    \n",
    "    Args:\n",
    "        vectorizer: HFTextVectorizer instance\n",
    "        \n",
    "    Returns:\n",
    "        SemanticCache instance\n",
    "    \"\"\"\n",
    "    print(\"Setting up semantic cache...\")\n",
    "    \n",
    "    cache = SemanticCache(\n",
    "        name=\"llmcache\",\n",
    "        vectorizer=vectorizer,\n",
    "        redis_url=REDIS_URL,\n",
    "        ttl=CACHE_TTL,\n",
    "        distance_threshold=CACHE_DISTANCE_THRESHOLD,\n",
    "        overwrite=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Cache created with TTL={CACHE_TTL}s, threshold={CACHE_DISTANCE_THRESHOLD}\")\n",
    "    return cache\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# CHATBOT CLASS WITH CACHE AND MEMORY\n",
    "# ============================================\n",
    "\n",
    "class ChatBot:\n",
    "    \"\"\"\n",
    "    Production-ready RAG chatbot with semantic caching and session memory\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        index: AsyncSearchIndex, \n",
    "        vectorizer: HFTextVectorizer,\n",
    "        cache: SemanticCache,\n",
    "        user: str\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize ChatBot\n",
    "        \n",
    "        Args:\n",
    "            index: AsyncSearchIndex for document search\n",
    "            vectorizer: HFTextVectorizer for embeddings\n",
    "            cache: SemanticCache for response caching\n",
    "            user: User identifier for session management\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cache = cache\n",
    "        self.user = user\n",
    "        \n",
    "        # Initialize session manager\n",
    "        self.session_manager = SemanticMessageHistory(\n",
    "            name=f\"chat_session_{user}\",\n",
    "            redis_url=REDIS_URL,\n",
    "            ttl=SESSION_TTL\n",
    "        )\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"total_queries\": 0,\n",
    "            \"avg_response_time\": 0\n",
    "        }\n",
    "        \n",
    "        print(f\"ChatBot initialized for user: {user}\")\n",
    "    \n",
    "    def embed_query(self, query: str):\n",
    "        \"\"\"Convert query to vector\"\"\"\n",
    "        return self.vectorizer.embed(query)\n",
    "    \n",
    "    async def retrieve_context(self, query_vector, num_results: int = NUM_RESULTS) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve relevant context from Redis\n",
    "        \n",
    "        Args:\n",
    "            query_vector: Embedded query vector\n",
    "            num_results: Number of chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            Combined context string\n",
    "        \"\"\"\n",
    "        results = await self.index.query(\n",
    "            VectorQuery(\n",
    "                vector=query_vector,\n",
    "                vector_field_name=\"text_embedding\",\n",
    "                num_results=num_results,\n",
    "                return_fields=[\"content\"],\n",
    "                return_score=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        context = \"\\n\\n\".join([result['content'] for result in results])\n",
    "        return context\n",
    "    \n",
    "    @staticmethod\n",
    "    async def generate_llm_response(\n",
    "        query: str, \n",
    "        context: str, \n",
    "        session: Optional[List] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate LLM response with context and conversation history\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context from documents\n",
    "            session: Conversation history\n",
    "            \n",
    "        Returns:\n",
    "            LLM-generated answer\n",
    "        \"\"\"\n",
    "        # Build conversation history\n",
    "        conversation = \"\"\n",
    "        if session:\n",
    "            for msg in session[-5:]:  # Last 5 turns only\n",
    "                role = msg.get(\"role\", \"unknown\")\n",
    "                content = msg.get(\"content\", \"\")\n",
    "                conversation += f\"{role.capitalize()}: {content}\\n\"\n",
    "        \n",
    "        # Construct prompt\n",
    "        prompt = f'''Use the provided context and conversation history to answer \n",
    "        the user's question. If the question refers to something from earlier in \n",
    "        the conversation, use that context. If you cannot answer based on the \n",
    "        provided information, say so.\n",
    "        \n",
    "        Context from documents:\n",
    "        {context}\n",
    "        \n",
    "        Conversation history:\n",
    "        {conversation if conversation else \"No previous conversation\"}\n",
    "        \n",
    "        Current question: {query}\n",
    "        \n",
    "        Answer:'''\n",
    "        \n",
    "        response = await openai.AsyncClient().chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2,\n",
    "            seed=42\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    async def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.session_manager.clear()\n",
    "        print(f\"Session history cleared for user: {self.user}\")\n",
    "    \n",
    "    async def answer_question(self, query: str) -> dict:\n",
    "        \"\"\"\n",
    "        Answer question with caching and session memory\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with answer and metadata\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.metrics[\"total_queries\"] += 1\n",
    "        \n",
    "        # Embed the query\n",
    "        query_vector = self.embed_query(query)\n",
    "        \n",
    "        # Check cache first\n",
    "        cached_result = self.cache.check(vector=query_vector)\n",
    "        \n",
    "        if cached_result:\n",
    "            # Cache hit\n",
    "            answer = cached_result[0]['response']\n",
    "            self.metrics[\"cache_hits\"] += 1\n",
    "            used_cache = True\n",
    "            \n",
    "            # Still add to session history\n",
    "            self.session_manager.add_messages([\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"answer\": answer,\n",
    "                \"used_cache\": True,\n",
    "                \"used_memory\": False,\n",
    "                \"response_time\": elapsed,\n",
    "                \"source\": \"cache\"\n",
    "            }\n",
    "        \n",
    "        # Cache miss - run full RAG\n",
    "        self.metrics[\"cache_misses\"] += 1\n",
    "        \n",
    "        # Get conversation history\n",
    "        session = self.session_manager.messages\n",
    "        used_memory = bool(session)\n",
    "        \n",
    "        # Retrieve context\n",
    "        context = await self.retrieve_context(query_vector)\n",
    "        \n",
    "        # Generate response\n",
    "        answer = await self.generate_llm_response(query, context, session)\n",
    "        \n",
    "        # Store in cache\n",
    "        self.cache.store(\n",
    "            prompt=query,\n",
    "            response=answer,\n",
    "            vector=query_vector\n",
    "        )\n",
    "        \n",
    "        # Add to session history\n",
    "        self.session_manager.add_messages([\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ])\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"used_cache\": False,\n",
    "            \"used_memory\": used_memory,\n",
    "            \"response_time\": elapsed,\n",
    "            \"source\": \"llm\"\n",
    "        }\n",
    "    \n",
    "    def get_metrics(self) -> dict:\n",
    "        \"\"\"Get performance metrics\"\"\"\n",
    "        cache_hit_rate = (\n",
    "            self.metrics[\"cache_hits\"] / self.metrics[\"total_queries\"] * 100\n",
    "            if self.metrics[\"total_queries\"] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **self.metrics,\n",
    "            \"cache_hit_rate\": f\"{cache_hit_rate:.1f}%\"\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# DEMO FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "async def demo_semantic_caching(chatbot: ChatBot):\n",
    "    \"\"\"Demonstrate semantic caching with similar queries\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEMO: Semantic Caching\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    queries = [\n",
    "        \"What are Nike's main revenue drivers?\",\n",
    "        \"What are Nike's primary sources of income?\",  # Semantically similar\n",
    "        \"Tell me about Nike's biggest revenue streams?\"  # Also similar\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(queries, 1):\n",
    "        print(f\"\\n--- Query {i} ---\")\n",
    "        print(f\"Question: {query}\")\n",
    "        \n",
    "        result = await chatbot.answer_question(query)\n",
    "        \n",
    "        print(f\"Answer: {result['answer'][:200]}...\")\n",
    "        print(f\"Source: {result['source']}\")\n",
    "        print(f\"Cache hit: {result['used_cache']}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "\n",
    "\n",
    "async def demo_session_memory(chatbot: ChatBot):\n",
    "    \"\"\"Demonstrate session memory with follow-up questions\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEMO: Session Memory\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Clear history for clean demo\n",
    "    await chatbot.clear_history()\n",
    "    \n",
    "    conversation = [\n",
    "        \"What was Nike's marketing strategy?\",\n",
    "        \"What role do athlete partnerships play in that?\",  # Follow-up\n",
    "        \"How much do they spend on it?\"  # Another follow-up\n",
    "    ]\n",
    "    \n",
    "    for i, query in enumerate(conversation, 1):\n",
    "        print(f\"\\n--- Turn {i} ---\")\n",
    "        print(f\"User: {query}\")\n",
    "        \n",
    "        result = await chatbot.answer_question(query)\n",
    "        \n",
    "        print(f\"Assistant: {result['answer'][:200]}...\")\n",
    "        print(f\"Used memory: {result['used_memory']}\")\n",
    "        print(f\"Response time: {result['response_time']:.3f}s\")\n",
    "\n",
    "\n",
    "async def demo_performance_comparison(chatbot: ChatBot):\n",
    "    \"\"\"Compare performance with and without optimizations\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DEMO: Performance Comparison\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Same query asked twice\n",
    "    query = \"What are Nike's operating segments?\"\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    \n",
    "    # First call (cache miss)\n",
    "    print(\"\\nFirst call (cache miss):\")\n",
    "    result1 = await chatbot.answer_question(query)\n",
    "    print(f\"Response time: {result1['response_time']:.3f}s\")\n",
    "    print(f\"Source: {result1['source']}\")\n",
    "    \n",
    "    # Second call (cache hit)\n",
    "    print(\"\\nSecond call (cache hit):\")\n",
    "    result2 = await chatbot.answer_question(query)\n",
    "    print(f\"Response time: {result2['response_time']:.3f}s\")\n",
    "    print(f\"Source: {result2['source']}\")\n",
    "    \n",
    "    # Calculate improvement\n",
    "    speedup = result1['response_time'] / result2['response_time']\n",
    "    print(f\"\\nSpeedup: {speedup:.1f}x faster\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main demo function\n",
    "    \n",
    "    Note: This assumes you have already:\n",
    "    1. Processed and chunked your documents\n",
    "    2. Generated embeddings\n",
    "    3. Created and loaded a Redis index\n",
    "    \n",
    "    See the basic RAG example for those steps.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Production RAG with Caching and Memory\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize components (assumes index already exists)\n",
    "    vectorizer = HFTextVectorizer(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        dims=384\n",
    "    )\n",
    "    \n",
    "    # Setup cache\n",
    "    cache = setup_semantic_cache(vectorizer)\n",
    "    \n",
    "    # Setup async index (assumes schema already created)\n",
    "    schema = {\n",
    "        \"index\": {\"name\": \"redisvl_rag\", \"prefix\": \"doc\"},\n",
    "        \"fields\": [\n",
    "            {\"name\": \"chunk_id\", \"type\": \"numeric\"},\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"text_embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": 384,\n",
    "                    \"algorithm\": \"flat\",\n",
    "                    \"distance_metric\": \"cosine\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    from redisvl.index import AsyncSearchIndex\n",
    "    async_index = AsyncSearchIndex.from_dict(schema, redis_url=REDIS_URL)\n",
    "    \n",
    "    # Initialize chatbot\n",
    "    chatbot = ChatBot(\n",
    "        index=async_index,\n",
    "        vectorizer=vectorizer,\n",
    "        cache=cache,\n",
    "        user=\"demo_user\"\n",
    "    )\n",
    "    \n",
    "    # Run demos\n",
    "    await demo_semantic_caching(chatbot)\n",
    "    await demo_session_memory(chatbot)\n",
    "    await demo_performance_comparison(chatbot)\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Final Metrics\")\n",
    "    print(\"=\" * 60)\n",
    "    metrics = chatbot.get_metrics()\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
