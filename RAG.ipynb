{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01841359-0a44-4dc1-a0c3-cc1fa89dcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from typing import List\n",
    "from redis import Redis\n",
    "from redisvl.utils.vectorize import HFTextVectorizer\n",
    "from redisvl.index import SearchIndex, AsyncSearchIndex\n",
    "from redisvl.query import VectorQuery\n",
    "from redisvl.redis.utils import array_to_buffer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import openai\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Redis connection settings\n",
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\")\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
    "\n",
    "# OpenAI settings\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Document processing settings\n",
    "CHUNK_SIZE = 2500\n",
    "CHUNK_OVERLAP = 0\n",
    "NUM_RESULTS = 5\n",
    "\n",
    "# ============================================\n",
    "# DOCUMENT PROCESSING\n",
    "# ============================================\n",
    "\n",
    "def load_and_chunk_document(file_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Load a PDF and split it into chunks\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        List of document chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading document: {file_path}\")\n",
    "    \n",
    "    # Initialize the PDF loader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    \n",
    "    # Create text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    \n",
    "    # Load and split the document\n",
    "    chunks = loader.load_and_split(text_splitter)\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EMBEDDING GENERATION\n",
    "# ============================================\n",
    "\n",
    "def create_vectorizer():\n",
    "    \"\"\"Initialize the HuggingFace vectorizer\"\"\"\n",
    "    return HFTextVectorizer(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        dims=384\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_embeddings(chunks: List, vectorizer: HFTextVectorizer) -> List:\n",
    "    \"\"\"\n",
    "    Generate vector embeddings for document chunks\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks\n",
    "        vectorizer: HFTextVectorizer instance\n",
    "        \n",
    "    Returns:\n",
    "        List of embeddings\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings...\")\n",
    "    \n",
    "    # Extract text content from chunks\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = vectorizer.embed_many(texts, as_buffer=False)\n",
    "    \n",
    "    print(f\"Generated {len(embeddings)} embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# REDIS SETUP\n",
    "# ============================================\n",
    "\n",
    "def create_redis_schema():\n",
    "    \"\"\"Define the Redis schema for document storage\"\"\"\n",
    "    return {\n",
    "        \"index\": {\n",
    "            \"name\": \"redisvl_rag\",\n",
    "            \"prefix\": \"doc\"\n",
    "        },\n",
    "        \"fields\": [\n",
    "            {\"name\": \"chunk_id\", \"type\": \"numeric\"},\n",
    "            {\"name\": \"content\", \"type\": \"text\"},\n",
    "            {\n",
    "                \"name\": \"text_embedding\",\n",
    "                \"type\": \"vector\",\n",
    "                \"attrs\": {\n",
    "                    \"dims\": 384,\n",
    "                    \"algorithm\": \"flat\",\n",
    "                    \"distance_metric\": \"cosine\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def setup_redis_index(schema: dict, redis_url: str) -> SearchIndex:\n",
    "    \"\"\"\n",
    "    Create and initialize Redis index\n",
    "    \n",
    "    Args:\n",
    "        schema: Redis schema definition\n",
    "        redis_url: Redis connection URL\n",
    "        \n",
    "    Returns:\n",
    "        SearchIndex instance\n",
    "    \"\"\"\n",
    "    print(\"Creating Redis index...\")\n",
    "    \n",
    "    index = SearchIndex.from_dict(schema, redis_url=redis_url)\n",
    "    index.create(overwrite=True, drop=True)\n",
    "    \n",
    "    print(\"Index created successfully\")\n",
    "    return index\n",
    "\n",
    "\n",
    "def load_data_to_redis(chunks: List, embeddings: List, index: SearchIndex):\n",
    "    \"\"\"\n",
    "    Load document chunks and embeddings into Redis\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks\n",
    "        embeddings: List of embeddings\n",
    "        index: SearchIndex instance\n",
    "    \"\"\"\n",
    "    print(\"Loading data into Redis...\")\n",
    "    \n",
    "    # Prepare data for Redis\n",
    "    data = [\n",
    "        {\n",
    "            'chunk_id': i,\n",
    "            'content': chunk.page_content,\n",
    "            'text_embedding': array_to_buffer(embeddings[i], dtype='float32')\n",
    "        } for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    # Load into Redis\n",
    "    index.load(data)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} documents into Redis\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# RAG PIPELINE FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def embed_query(query: str, vectorizer: HFTextVectorizer):\n",
    "    \"\"\"\n",
    "    Convert user query to vector representation\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        vectorizer: HFTextVectorizer instance\n",
    "        \n",
    "    Returns:\n",
    "        Query embedding vector\n",
    "    \"\"\"\n",
    "    return vectorizer.embed(query)\n",
    "\n",
    "\n",
    "async def retrieve_context(\n",
    "    async_index: AsyncSearchIndex, \n",
    "    query_vector, \n",
    "    num_results: int = NUM_RESULTS\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant context from Redis using vector search\n",
    "    \n",
    "    Args:\n",
    "        async_index: AsyncSearchIndex instance\n",
    "        query_vector: Embedded query vector\n",
    "        num_results: Number of chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Combined context string\n",
    "    \"\"\"\n",
    "    results = await async_index.query(\n",
    "        VectorQuery(\n",
    "            vector=query_vector,\n",
    "            vector_field_name=\"text_embedding\",\n",
    "            num_results=num_results,\n",
    "            return_fields=[\"content\"],\n",
    "            return_score=True\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Combine retrieved chunks into context\n",
    "    context = \"\\n\\n\".join([result['content'] for result in results])\n",
    "    \n",
    "    return context\n",
    "\n",
    "\n",
    "def promptify(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM with context and query\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        context: Retrieved context from Redis\n",
    "        \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    return f'''Use the provided context below derived from documents to answer \n",
    "    the user's question. If you can't answer the user's question based on the \n",
    "    context, do not guess. Respond with \"I don't have enough information to \n",
    "    answer that question.\"\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:'''\n",
    "\n",
    "\n",
    "async def generate_llm_response(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using OpenAI's LLM\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        context: Retrieved context\n",
    "        \n",
    "    Returns:\n",
    "        LLM-generated answer\n",
    "    \"\"\"\n",
    "    prompt = promptify(query, context)\n",
    "    \n",
    "    response = await openai.AsyncClient().chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "async def answer_question(\n",
    "    async_index: AsyncSearchIndex, \n",
    "    query: str, \n",
    "    vectorizer: HFTextVectorizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: embed query, retrieve context, generate answer\n",
    "    \n",
    "    Args:\n",
    "        async_index: AsyncSearchIndex instance\n",
    "        query: User's question\n",
    "        vectorizer: HFTextVectorizer instance\n",
    "        \n",
    "    Returns:\n",
    "        Final answer from the LLM\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    \n",
    "    # Step 1: Embed the query\n",
    "    query_vector = embed_query(query, vectorizer)\n",
    "    \n",
    "    # Step 2: Retrieve matching context from Redis\n",
    "    context = await retrieve_context(async_index, query_vector)\n",
    "    \n",
    "    # Step 3: Generate answer using LLM\n",
    "    answer = await generate_llm_response(query, context)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution function demonstrating the complete RAG pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RAG System with Redis and OpenAI\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Load and chunk the document\n",
    "    pdf_path = \"resources/nke-10k-2023.pdf\"  # Update with your PDF path\n",
    "    chunks = load_and_chunk_document(pdf_path)\n",
    "    \n",
    "    # Step 2: Initialize vectorizer and generate embeddings\n",
    "    vectorizer = create_vectorizer()\n",
    "    embeddings = generate_embeddings(chunks, vectorizer)\n",
    "    \n",
    "    # Step 3: Setup Redis\n",
    "    schema = create_redis_schema()\n",
    "    sync_index = setup_redis_index(schema, REDIS_URL)\n",
    "    \n",
    "    # Step 4: Load data into Redis\n",
    "    load_data_to_redis(chunks, embeddings, sync_index)\n",
    "    \n",
    "    # Step 5: Create async index for querying\n",
    "    async_index = AsyncSearchIndex.from_dict(schema, redis_url=REDIS_URL)\n",
    "    \n",
    "    # Step 6: Test the RAG system with sample queries\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Testing RAG System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    sample_queries = [\n",
    "        \"What were Nike's profit margins and company performance?\",\n",
    "        \"What are the main risks facing Nike's business?\",\n",
    "        \"What is Nike's strategy for digital transformation?\"\n",
    "    ]\n",
    "    \n",
    "    for query in sample_queries:\n",
    "        answer = await answer_question(async_index, query, vectorizer)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\nRAG System test complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the async main function\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
